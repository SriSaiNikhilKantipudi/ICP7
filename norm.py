# -*- coding: utf-8 -*-
"""Norm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ucEseIym84fwIxUW4f5C7QC5beb_GmB_
"""

import requests
from bs4 import BeautifulSoup
response = requests.get('https://en.wikipedia.org/wiki/Google')
content = BeautifulSoup(response.content, "html.parser")
text = content.get_text()
f = open('input.txt', 'w',encoding='utf-8')
f.write(text)

import nltk
from nltk.util import ngrams
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk import wordpunct_tokenize, pos_tag, ne_chunk

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('maxent_ne_chunker')
nltk.download('words')

wtokens=nltk.word_tokenize(text)
stokens=nltk.sent_tokenize(text)
for w in wtokens:
  print(w)

postxt=nltk.pos_tag(wtokens)
print(postxt)

pstemmer=PorterStemmer()
for w in wtokens:
  print(pstemmer.stem(w))

lemmatizer=WordNetLemmatizer()
for w in wtokens:
  print(lemmatizer.lemmatize(w))

print(ne_chunk(pos_tag(wordpunct_tokenize(text))))

trigrams=list(ngrams(wtokens,3))
print(trigrams)